---
title: "Quanteda_Chapter_5"
author: "Ethan Mah"
date: "2024-04-24"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(quanteda)
require(quanteda.textstats)
require(readtext)
require(quanteda.corpora)
options(width = 110)
```

# Advanced Operations
# Compute Similarity Between Authors

We can compute the similarities between authors by grouping their documents and comparing them with all other authors. 

In this example, we group Twitter posts by handle names and compute similarities between the users. Import Tweets from JSON (.json) file. twitter.json is located in data directory of this tutorial package.

```{r}
data_twitter <- readtext("data/twitter.json", source = "twitter")
corp_tweets <- corpus(data_twitter)
dfmat_tweets <- corp_tweets %>%
  tokens(remove_punct = TRUE, remove_url = TRUE, remove_symbols = TRUE) %>%
  dfm() %>%
  dfm_remove(pattern = c("*.tt", "*.uk", "*.com", "rt", "#*", "@*")) %>%
  dfm_remove(pattern = stopwords("en"))
```

```{r}
ndoc(dfmat_tweets)
topfeatures(dfmat_tweets)
```

We group documents by Twitter handle names (screen_name), and remove rare (less than 10 times) and short (one character) features, and keep only users with more than 50 tokens in total.

```{r}
dfmat_users <- dfm_group(dfmat_tweets, groups = screen_name)
ndoc(dfmat_users)
dfmat_users <- dfmat_users %>% 
    dfm_select(min_nchar = 2) %>% 
    dfm_trim(min_termfreq = 10) 
dfmat_users <- dfmat_users[ntoken(dfmat_users) > 50,]
```

Calculate user-user similarity using textstat_dist().

```{r}
tstat_dist <- as.dist(textstat_dist(dfmat_users))
user_clust <- hclust(tstat_dist)
plot(user_clust)
```
# Compound Multi-word Expressions 

We can compound multi-word expressions through collocation analysis (see Chpt 4). 
In this example, we will identify sequences of capitalized words and compound them as proper names, which are important linguistic features of newspaper articles. This corpus contains 6,000 Guardian news articles from 2012 to 2016.

```{r}
corp_news <- download("data_corpus_guardian") # 6,000 Guardian news articles from 2012 to 2016
# remove punctuation marks, symbols in tokens(), stopwords in tokens_remove() with padding = TRUE to keep original positions of tokens
toks_news <- tokens(corp_news, remove_punct = TRUE, remove_symbols = TRUE, padding = TRUE) %>% 
    tokens_remove(stopwords("en"), padding = TRUE)
# One of the most common type of multi-word expressions is proper names, which we can select simply based on capitalization in English texts.
toks_news_cap <- tokens_select(toks_news, 
                               pattern = "^[A-Z]",
                               valuetype = "regex",
                               case_insensitive = FALSE, 
                               padding = TRUE)

tstat_col_cap <- textstat_collocations(toks_news_cap, min_count = 10, tolower = FALSE)
head(tstat_col_cap, 20)
```
We will only compound strongly associated multi-word expressions here by subsetting tstat_col_cap with the z-score (z > 3).
```{r}
toks_comp <- tokens_compound(toks_news, pattern = tstat_col_cap[tstat_col_cap$z > 3,], 
                             case_insensitive = FALSE)
kw_comp <- kwic(toks_comp, pattern = c("London_*", "British_*"))
head(kw_comp, 10)
```

# Apply Dictionary to Specific Contexts
We can detect occurrences of words in specific contexts by selectively applying dictionary. In this example, we will apply a sentiment dictionary to segments of news articles that mentions the (British) government.

Tokenize texts and select tokens surrounding keywords related to the government using tokens_keep().

```{r}
corp_news <- download("data_corpus_guardian") # 6,000 Guardian news articles from 2012 to 2016
# tokenize corpus
toks_news <- tokens(corp_news, remove_punct = TRUE)

# get relevant keywords and phrases
gov <- c("government", "cabinet", "prime minister")

# only keep tokens specified above and their context of ±10 tokens
# note: use phrase() to correctly score multi-word expressions
toks_gov <- tokens_keep(toks_news, pattern = phrase(gov), window = 10)
```

Apply the Lexicoder Sentiment Dictionary to the selected contexts using tokens_lookup().
The LSD is a bag-of-words dictionary designed for the automated coding of sentiment in news coverage, legislative speech and other text.

```{r}
lengths(data_dictionary_LSD2015)
# select only the "negative" and "positive" categories
data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]

toks_gov_lsd <- tokens_lookup(toks_gov, dictionary = data_dictionary_LSD2015_pos_neg)

# create a document document-feature matrix and group it by day
dfmat_gov_lsd <- dfm(toks_gov_lsd) %>% 
  dfm_group(groups = date)
```

```{r}
matplot(dfmat_gov_lsd$date, dfmat_gov_lsd, type = "l", lty = 1, col = 1:2,
        ylab = "Frequency", xlab = "")
grid()
legend("topleft", col = 1:2, legend = colnames(dfmat_gov_lsd), lty = 1, bg = "white")
```
We can compute daily sentiment scores by taking the difference between the frequency of positive and negative words.

```{r}
plot(dfmat_gov_lsd$date, dfmat_gov_lsd[,"positive"] - dfmat_gov_lsd[,"negative"], 
     type = "l", ylab = "Sentiment", xlab = "")
grid()
abline(h = 0, lty = 2)
```

We can apply kernel smoothing to show the trend more clearly.

```{r}
dat_smooth <- ksmooth(x = dfmat_gov_lsd$date, 
                      y = dfmat_gov_lsd[,"positive"] - dfmat_gov_lsd[,"negative"],
                      kernel = "normal", bandwidth = 30)
plot(dat_smooth$x, dat_smooth$y, type = "l", ylab = "Sentiment", xlab = "")
grid()
abline(h = 0, lty = 2)
```
# Identify Related Words of Keywords
We can identify related words of keywords based on their distance in the documents. In this example, we created a list of words related to the European Union by comparing frequency of words inside and outside of their contexts.

```{r}
corp_news <- download("data_corpus_guardian") # 6,000 Guardian news articles from 2012 to 2016
# tokenize corpus
toks_news <- tokens(corp_news, remove_punct = TRUE)
```

We will select two tokens objects for words inside and outside of the 10-word windows of the keywords (eu).

```{r}
corp_news <- download("data_corpus_guardian") # 6,000 Guardian news articles from 2012 to 2016
# tokenize corpus
toks_news <- tokens(corp_news, remove_punct = TRUE)
eu <- c("EU", "europ*", "european union")
toks_inside <- tokens_keep(toks_news, pattern = eu, window = 10)
toks_inside <- tokens_remove(toks_inside, pattern = eu) # remove the keywords
toks_outside <- tokens_remove(toks_news, pattern = eu, window = 10)
```

We can compute words’ association with the keywords using textstat_keyness().

```{r}
dfmat_inside <- dfm(toks_inside)
dfmat_outside <- dfm(toks_outside)

tstat_key_inside <- textstat_keyness(rbind(dfmat_inside, dfmat_outside), 
                                     target = seq_len(ndoc(dfmat_inside)))
head(tstat_key_inside, 50)
```